---
title: "ExerciseWk2"
author: "Eli Clapper"
date: "29/09/2021"
output: html_document
---

Lets run a Gibbs sampler for a linear regression where we want to regress `IQ` on `Reading` and `Math` abilities and compare the output of our Gibbs sampler to the output of the `stats::lm()` function.

First we simulate a dataframe using `library(faux)` that allows drawing variables from
a multivariate distribution and allows controlling how much variables are correlated.

```{r}
suppressWarnings(suppressMessages(library(faux)))
#library to easily obtain multivariate normally distributed correlated variables

#set seed for reproducibility
set.seed(42) 

#generate data
dat <- rnorm_multi(n = 1000, #generate 1000 observations 
                   mu = c(100, 25, 25), #these are the means for IQ, Reading and Math respectively
                   sd = c(15, 5, 5), #the standard deviations
                   r = c(0.65, 0.7, 0.20), #correlations between the variables
                   varnames = c("IQ", "Reading", "Math")) #give the variables names
```


If we run a linear regression we find that both `Reading` and `Math abilities` significantly explain 
`IQ` score, we find $b_{intercept} = 17.08$, $b_{read} = 1.58$ and $b_{math} = 1.73$


```{r}
dat %$%
  lm(IQ ~ Reading + Math) %>%
  summary()
```


If we then run a Gibbs sampler with uninformative priors, we must find that the mean of the MC chains for $b_{intercept}$, $b_{Read}$ and $b_{Math}$ are roughly the same. In order to run the Gibbs sampler, we first  have to specify hyperparameters for uniformative priors and well as initial values for the chain to start the $b$ values at. The hyperparameter must not be random, because we want to make sure they represent non-information. The initial values can be randomized however.


```{r}
hyperparameters <- list(m00=0,t00=1000, m01=0, t01=1000, m02=0,t02=1000, alpha = 0.001, beta = 0.001)

set.seed(42)
inits <- list(b0=sample(1:100, 1), b1=sample(1:100, 1), b2=sample(1:100, 1), v=sample(1:100, 1))
```


Below is a function that I created in our Bayesian statistics course. It is a Gibbs sampler for a linear regression. I did adjust it a little, because the original also had a Metropolis Hastings step which I now excluded. The output is the forced joint posterior density of `IQ`, `Reading` and `Math`, these are chains containing values that exist within this density for every parameter. There is an inbuilt option in this function to specify a seed.


```{r}
joint.posterior <- function(y, x1, x2, hp = list(), inits, niter = 10000, burnin = niter/5, seed = 42){
  n <- length(y) #number of observations
  
  # create storage for the parameters to be estimated
  b0 <- rep(0, niter)
  b1 <- rep(0, niter)
  b2 <- rep(0, niter)
  v <- rep(0, niter)
  
  # set initial values
  b0[1] <- inits[['b0']] #initial values that were passed as an argument in the function
  b1[1] <- inits[['b1']]
  b2[1] <- inits[['b2']]
  v[1] <- inits[['v']]

  
  #Gibbs sampler
  set.seed(seed) #set seed for reproducibility
  for(i in 2:niter){
    b0[i] <- rnorm(n = 1, mean = ((sum(y - b1[i-1]*x1 - b2[i-1]*x2) / v[i-1]) + (hp[['m00']]/hp[['t00']])) / ((n/v[i-1]) + (1 / hp[['t00']])), sd = sqrt(1 / ((n/v[i-1]) + (1 / hp[['t00']]))))

    b1[i] <- rnorm(1, mean = (((sum(x1 * (y- b0[i]-b2[i-1]*x2))) / (v[i-1])) + (hp[['m01']]/hp[['t01']])) / ((sum(x1^2)/v[i-1]) + (1/hp[['t01']])), sd = sqrt(1/((sum(x1^2)/v[i-1]) + (1/hp[['t01']]))))

    
    b2[i] <- rnorm(1, mean = (((sum(x2 * (y- b0[i]-b1[i]*x1))) / (v[i-1])) + (hp[['m02']]/hp[['t02']])) / ((sum(x2^2)/v[i-1]) + (1/hp[['t02']])), sd = sqrt(1/((sum(x2^2)/v[i-1]) + (1/hp[['t02']]))))
    
    v[i] <- 1/rgamma(1, shape = (n/2) + hp[['alpha']], rate = (sum((y - (b0[i] + b1[i] * x1 + b2[i]*x2))^2) / 2) + hp[['beta']])
  } 
    
  
  #Store results in a dataframe with specified burnin period
  result_beta <- cbind(b0, b1, b2, v) #raw values
  colnames(result_beta) <- c('Intercept', 'Reading', 'Math', 'Variance') #give appropriate names
  result <- as.data.frame(result_beta[-(1:burnin),]) #leave us with result minus burnin values
  return(result)
}

```


If we run the function using the same datapoints we used in the `lm()` function, and take the mean over the chains, we should obtain roughly the same estimates as we got in the `lm()` function.


```{r}
res <- joint.posterior(y = dat$IQ, x1 = dat$Reading, x2 = dat$Math, hp = hyperparameters, inits = inits, niter = 10000, seed = 42)

apply(res[,1:3], 2, mean)
```


That is exactly what we see with $b_{intercept} = 17.16}$, $b_{read} = 1.58$ and $b_{math} = 1.73$.


Finally, below the session information.
```{r}
sessionInfo()
```